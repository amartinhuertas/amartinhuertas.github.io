<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/jtd.css"> <link rel=icon  href="/assets/favicon.ico"> <title>Franklin Example</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/" class=title > Alberto F. Mart√≠n </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item active"><a href="/" class="menu-list-link active">Home</a> <!-- <li class="menu-list-item {{ispage menu1/*}}active{{end}}"><a href="/menu1/" class="menu-list-link {{ispage menu1}}active{{end}}">Curriculum Vitae</a> <li class="menu-list-item {{ispage menu2/*}}active{{end}}"><a href="/menu2/" class="menu-list-link {{ispage menu2}}active{{end}}">Research</a> <li class="menu-list-item {{ispage menu3/*}}active{{end}}"><a href="/menu3/" class="menu-list-link {{ispage menu3}}active{{end}}">Software</a> <li class="menu-list-item {{ispage menu4/*}}active{{end}}"><a href="/menu4/" class="menu-list-link {{ispage menu4}}active{{end}}">Publications</a> <ul class="menu-list-child-list "> <li class="menu-list-item "><a href="#" class=menu-list-link >Submenu</a> </ul> --> </ul> </div> <div class=footer > <div class=author__urls-wrapper > </div> </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="https://github.com/amartinhuertas">GitHub</a> &nbsp &nbsp &nbsp &nbsp <a id=ResearchGate  href="https://www.researchgate.net/profile/Alberto-Martin-10">Research Gate</a> &nbsp &nbsp &nbsp &nbsp <a id=GoogleScholar  href="https://scholar.google.es/citations?user=CumvSwwAAAAJ">Google Scholar</a> &nbsp &nbsp &nbsp &nbsp <a id=MonashProfile  href="https://research.monash.edu/en/persons/alberto-f-martin">Monash Profile</a> </div> <div class=franklin-content ><h1 id=research_profile_summary ><a href="#research_profile_summary" class=header-anchor >Research Profile Summary</a></h1> <p>I have specialized in high performance computational mathematics, within the broad field of CSE, with a particular focus on the numerical solution of PDEs. As a researcher within this field, I have acquired expertise in the development of advanced FE discretization schemes and solution methods for the HPC modeling of large-scale problems of scientific and engineering relevance, while putting special emphasis on scientific software design innovations as a key output of my research. My contributions so far to this field span: </p> <ol> <li><p>The design and mathematical analysis of new, application-tailored FE discretizations and solution methods, or innovative algorithmic adaptations of state-of-the-art ones towards different goals. </p> <li><p>The efficient parallel implementation of these in the form of HPC software packages provided to the general CSE community. </p> <li><p>The application of the advances of 1. and 2. for the realistic computer simulation of real-world challenges in the sciences and engineering, in collaboration with application-problem specialists, and/or .private sector companies.</p> </ol> <p>As output of my research, among others, I have co-authored 24 research articles published in highly ranked peer-reviewed JCR journals &#40;23 Q1-ranked&#41;, 1 article published in a refereed collection, and 6 articles published as refereed book chapters &#40;see below for further reference&#41;.</p> <h2 id=my_10_selected_research_achievements ><a href="#my_10_selected_research_achievements" class=header-anchor >My 10 selected research achievements</a></h2> <ol> <li><p>I have developed a novel bulk-asynchronous, fully-distributed, communicator-aware, inter-level overlapped, and recursive algorithmic adaptation of MultiLevel BDDC preconditioners that efficiently scales up to the 458K cores of the IBM BG/Q supercomputer installed in JSC, Germany, in the solution of linear elliptic PDE problems &#40;e.g., Poisson or Elasticity&#41; with dozens of billions of unknowns. Thanks to these outstanding results &#40;unprecedented in the literature for domain decomposition techniques&#41;, <code>FEMPAR</code> &#40;see item below&#41; has been qualified for High-Q club status <span class=bibref >(<a href="#brommel_juqueen_2015">1</a>)</span>, a distinction that the Juelich Supercomputing Center &#40;Germany&#41; awards to the most scalable EU codes. This work was published in a series of two papers at SIAM Journal of Scientific Computing <span class=bibref >(<a href="#badia_highly_2014">2</a>, <a href="#badia_multilevel_2016">3</a>)</span>.</p> <li><p>I have addressed in a rigorous way the development of algorithms and data structures for parallel adaptive FE analysis on tree-based meshes endowed with SFC. The literature clearly failed to explain when and why the parallel algorithms and data structures required to support generic conforming FE discretizations atop tree-based adaptive meshes are correct. To solve this issue, I have inferred results based on mathematical propositions and proofs, yielding the &#40;correctness of the&#41; parallel algorithms in the framework. I wrote the MPI-parallel implementation of these algorithms, which is is available at <code>FEMPAR</code>. A strong scaling study of this implementation when applied to Poisson and Maxwell problems reveals remarkable scalability up to 32.2K CPU cores and 482.2M DOFs on the Marenostrum IV supercomputer. Besides, a comparison of <code>FEMPAR</code> performance with that of the state-of-the art <code>deal.II</code> software, reveals at least competitive performance, and at most factor 2-3 improvements on a massively parallel supercomputer. This work has been recently published at SIAM Journal on Scientific Computing <span class=bibref >(<a href="#badia2019a">4</a>)</span>.</p> <li><p>I have contributed to balancing domain decomposition solvers for large-scale PDEs in the form of new numerical algorithmic inventions and/or adaptation of existing ones towards different goals. In particular: &#40;a&#41; In a paper published at the <em>International Journal of Numerical methods in Engineering</em> <span class=bibref >(<a href="#badia_enhanced_2013">5</a>)</span>, I have rehabilitated the Balancing Neumann-Neumann preconditioner as an efficient preconditioning strategy for large-scale fluid and solid mechanics simulations. &#40;b&#41; In a paper published at the <em>Journal of Scientific Computing</em> <span class=bibref >(<a href="#badia2018pb">6</a>)</span>, I have developed a new variant of BDDC for multi-material problems. The new variant, grounded on a material-based aggregation technique, does not require expensive eigenvalue solvers like the mainstream ones based on adaptive selection of constraints, and it has been shown to be 8x times faster &#40;with a problem with half billion DOFs on 8.2K cores&#41;, while consuming significantly less memory resources, compared against a state-of-the-art, highly efficient MPI implementation of an adaptive-coarse-space BDDC preconditioner implemented in the PETSc software package from ANL.&#125;&#40;c&#41; In a paper published at <em>Parallel Computing</em> <span class=bibref >(<a href="#badia_scalability_2015">7</a>)</span>, I have developed an overlapped coarse/fine-grid message-passing implementation of inexact BDDC solvers that is able to boost their scalability on petascale computers. &#40;d&#41; In a paper published at <em>Applied Mathematical Letters</em> <span class=bibref >(<a href="#badia2019">8</a>)</span>, I have developed an enhanced variant of the original BDDC preconditioner&#125; that is able to eliminate the condition number matrix dependence on the ratio among the subdomain and mesh resolutions.</p> <li><p>I have developed a new embedded FE formulation for elliptic PDEs, the so-called, AgFEM method <span class=bibref >(<a href="#badia_aggregated_2017">9</a>)</span>, which is able to solve the conditioning and stability issues of embedded FE methods due to the so-called small cut cell problem using cell agglomeration techniques. The new formulation enjoys the following unique properties, compared to other solutions available in the literature: &#40;1&#41; it allows one to leverage existing parallel algebraic multigrid solvers for solving the underlying systems of linear algebraic equations at large scales; &#40;2&#41; it does not modify the underlying physical problem via stabilization parameters; &#40;3&#41; it is applicable to both continuous and discontinuous Galerkin formulations. The new method <span class=bibref >(<a href="#badia_aggregated_2017">9</a>)</span> was published at <em>Computer Methods in Applied Mechanics and Engineering</em>. Recently, I have extended the method to incompressible flow problems &#40;i.e., the Stokes equations&#41;, while being able to demonstrate that it is possible to build mixed aggregated FE pairs &#40;plus face stabilization&#41; in order to enjoy &#40;1&#41;-&#40;3&#41; for such kind of problems. This latter work was published at SIAM Journal on Scientific Computing <span class=bibref >(<a href="#badia_stokes_2018">10</a>)</span>. </p> <li><p>I have developed a novel tool for the simulation of the non-linear eddy current, AC loss large-scale modelling of HTS tapes and bulks. The hallmark of this tool, compared to any other currently available in the HTS modelling community, is: &#40;1&#41; its ability to combine arbitrary-order Edge FEs for the Maxwell Equations with AMR on octree-based meshes while exploiting message-passing parallelism in all stages of the simulation pipeline <span class=bibref >(<a href="#olm2019">11</a>)</span>, and &#40;2&#41; a novel 2-level BDDC preconditioner that I have developed for the linearized discrete Maxwell Equations with heterogeneous materials able to scale up to dozens of thousands for computational cores <span class=bibref >(<a href="#badia2019maxwell">12</a>)</span>. The numerical tool has been validated experimentally against the so-called Hall probe mapping experiment and it is already providing insightful understanding of the electromagnetic behavior, under different design parameters &#40;e.g., size and shape&#41;, of the HTS tapes manufactured by the company OXOLUTIA S.L. All these results have been published in a paper <span class=bibref >(<a href="#olm2019">11</a>)</span> at <em>Computer Physics Communications</em>, and the new 2-level BDDC preconditioner, at the <em>Finite Elements in Analysis and Design</em> journal <span class=bibref >(<a href="#badia2019maxwell">12</a>)</span>.</p> <li><p>I have developed a pair of new stabilized, variational multiscale FE discretization schemes for the thermally-coupled incompressible inductionless MHD system of PDE equations. These new FE schemes are accompanied with new efficient, algorithmically scalable preconditioners tailored for the resulting discrete operators based on a recursive use of block incomplete LU factorization. These preconditioners have been shown to keep the condition number of the multi-physics systems of equations bounded by a constant independent of the mesh resolution and number of processors. The combination of all the novel algorithms developed within this research line has enabled large-scale realistic BB simulations in nuclear fusion reactors up to scales and &#40;high&#41; Hartmann numbers unprecedented in the computational fusion community. In particular, on a simulation with a 100 million unstructured tetrahedral mesh on 4096 CPU cores of the MN-III supercomputer &#40;at BSC-CNS&#41;, with a time step size as large as 0.025 secs, I was able to provide insightful simulation results to the MHD simulation of a dual-coolant liquid metal blanket designed by the Spanish BB Technology Programme TECNOFUS. This work was published at the Journal of Computational Physics~<span class=bibref >(<a href="#badia2019a">4</a>)</span>.</p> <li><p>I have recently addressed the development the message-passing variants of the algorithms involved in all stages of the AgFEM method &#40;e.g., construction of cell aggregates, set up of ill-posed DOF constraints, resolution of constraints during parallel FE assembly, etc.&#41; Up to the fact that the processors might require to retrieve from remote processors the roots of those cell aggregates which are split among processors, this work has proven the AgFEM method to be a method very amenable to distributed-memory parallelization. In particular, it can be implemented using standard tools in parallel FE libraries, such as ghost cells nearest neighbour exchanges. I have implemented these algorithms in <code>FEMPAR</code> using MPI for inter-processor communications. Their high appeal at large scales has been demonstrated with a comprehensive weak scaling test up to 16K cores and up to nearly 300M DOFs and a billion cells in the Marenostrum-IV supercomputer using the Poisson equation on complex 3D domains as model problem. To the best of my knowledge, this is the first time that embedded methods are successfully applied to such large scales. This work has been recently published at a JCR Q1-ranked journal in the field &#40;see reference <span class=bibref >(<a href="#verdugo2019">13</a>)</span> for more details&#41;.</p> <li><p>During a period of 10 years &#40;2012-2021&#41;, I have provided to a group of 10 researchers &#40;on average&#41; access to 38 Million core-hours on different world-class supercomputing facilities &#40;HELIOS, HPC-FF, JUROPA, JUQUEEN, HERMIT, CURIE, MARENOSTRUM II-IV, MINOTAURO, FERMI, HLRN-III, GADI&#41;. I have achieved this by preparing winning project access proposals: 28x 4-month Spanish Supercomputing Network &#40;RES&#41; project proposals, 3x 6-month EU-PRACE preparatory access project proposals, 1x EU-PRACE TIER-0 1-year project proposal, 2x 1-year German Gauss Centre of Supercomputing project proposals, and 1x 1-year Australian NCMAS project proposal. </p> <li><p>I am co-founder, main software architect and project coordinator of <code>FEMPAR</code>, an open-source, HPC, hybrid MPI&#43;OpenMP parallel, scientific software package for the numerical modelling of problems governed by PDEs on HPC platforms &#40;from multi-core based clusters, to high-end petascale supercomputers&#41;. <code>FEMPAR</code> has been successfully used in 40x JCR Q1-ranked research papers on different topics and application areas: simulation of turbulent flows and stabilized FE methods , MHD, monotonic FEs, unfitted FEs and embedded boundary methods, AMR, AM and HTS simulations, and scientific software engineering. It has also been used for the highly efficient implementation of DD solvers and block preconditioning techniques. Its users/developers span different research groups on national and international-level institutions, including UPC, CIMNE, ICMAB-CSIC, CIEMAT, ICTJA-CSIC, Czech Academy of Sciences &#40;Czech Republic&#41;, Sandia National Labs &#40;EEUU&#41;, North Carolina State University &#40;USA&#41;, Duy Tan University &#40;Vietnam&#41;, Monash University &#40;Australia&#41;, and l‚ÄôEcole Politechnique &#40;Paris&#41;. Besides, it has been a crucial tool for the successful execution of several high-quality EU-funded projects, namely, 1x ERC starting grant, 2x ERC PoC projects, 1x EU-FP7 project, and 3x H2020 projects. I have presented the software engineering inventions which lay the foundations of <code>FEMPAR</code> in a series of 4x papers <span class=bibref >(<a href="#badia2020fempar">14</a>, <a href="#olm2019b">15</a>, <a href="#badia_fempar_2017">16</a>, <a href="#badia_implementation_2013">17</a>)</span> including a major 76-page manuscript that was published at the <em>Archives of Computational Methods in Engineering</em> <span class=bibref >(<a href="#badia_fempar_2017">16</a>)</span> &#40;ranked 5/106 in the area of Computer Science, Interdisciplinary applications. Source: 2018 JCR edition&#41;.</p> <li><p>In <span class=bibref >(<b>??</b>)</span>, I have addressed the challenge of bridging unfitted FE methods and adaptive mesh refinement at large scales. To this end, I have developed &#40;and implemented&#41; a parallel message-passing adaptive method that combines the AgFEM with parallel AMR on octree-based meshes. Mathematical analysis and numerical experiments demonstrate its optimal mesh adaptation capability, robustness to cut location and parallel efficiency, on classical Poisson <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">hp</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">p</span></span></span></span>-adaptivity benchmarks. This work opens the path to functional and geometrical error-driven dynamic mesh adaptation with AgFEM in large-scale realistic scenarios. Likewise, it can offer guidance for bridging other scalable unfitted methods and parallel AMR. Besides, in <span class=bibref >(<b>??</b>)</span>, with my post-doc, I have extended this method to large-scale nonlinear problems in solid mechanics. These two algorithms, together with other modelling and numerical discretization inventions, lay the foundations of an accurate computational model for the simulation of metal AM processes. Given a set of material and printing process parameters, the outcome with relevant industrial interest is the capability to predict potential printing defects, such as excess or lack of powder fusion. This sort of insightful information is crucial for application problem experimentalists, as, e.g., those at the Monash Centre for AM, with whom I collaborate. The advances so far in this regard have been published in two Q1-ranked JCR research journals~<span class=bibref >(<a href="#neiva2020">18</a>, <a href="#neiva2019a">19</a>)</span>.</p> </ol> <h2 id=references ><a href="#references" class=header-anchor >References</a></h2> <div class=bibrefs ><a id=brommel_juqueen_2015  class=anchor ></a> <ol> <li><p>Br√∂mmel, B. J. N. Wylie, and W. Frings. JUQUEEN Extreme Scaling Workshop 2015. Technical Report FZJ-2015-01645, FZJ-JSC-IB-2015-01, J√ºlich Supercomputing Center, 2015.</p> </ol> <a id=badia_highly_2014  class=anchor ></a> <ol start=2 > <li><p>S. Badia, A. <strong>F. Martin</strong>, and J. Principe. A Highly Scalable Parallel Implementation of Balancing Domain Decomposition by Constraints. <em>SIAM Journal on Scientific Computing</em>, 36&#40;2&#41;:C190‚ÄìC218, 2014. <a href="https://dx.doi.org/ 10.1137/130931989">DOI: 10.1137/130931989</a>.</p> </ol> <a id=badia_multilevel_2016  class=anchor ></a> <ol start=3 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong> and J. Principe. Multilevel Balancing Domain Decomposition at Extreme Scales. <em>SIAM Journal on Scientific Computing</em>, 38&#40;1&#41;:C22-C52, 2016. <a href="https://dx.doi.org/ 10.1137/15M1013511">DOI: 10.1137/15M1013511</a>.</p> </ol> <a id=badia2019a  class=anchor ></a> <ol start=4 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, E. Neiva, and F. Verdugo. A Generic Finite Element Framework on Parallel Tree-Based Adaptive Meshes. <strong>SIAM Journal on Scientific Computing</strong>, 42&#40;6&#41;:C436‚ÄìC468, 2020. <a href="https://dx.doi.org/ 10.1137/20M1328786">DOI: 10.1137/20M1328786</a>.</p> </ol> <a id=badia_enhanced_2013  class=anchor ></a> <ol start=5 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and J. Principe. Enhanced balancing Neumann-Neumann preconditioning in computational fluid and solid mechanics. <em>International Journal for Numerical Methods in Engineering</em>, 96&#40;4&#41;:203‚Äì230, 2013. <a href="https://dx.doi.org/ 10.1002/nme.4541">DOI: 10.1002/nme.4541</a>.</p> </ol> <a id=badia2018pb  class=anchor ></a> <ol start=6 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and H. Nguyen. Physics-Based Balancing Domain Decomposition by Constraints for Multi-Material Problems. <em>Journal of Scientific Computing</em>, 79&#40;2&#41;:718‚Äì747, 2019. <a href="https://dx.doi.org/ 10.1007/s10915-018-0870-z">DOI: 10.1007/s10915-018-0870-z</a>.</p> </ol> <a id=badia_scalability_2015  class=anchor ></a> <ol start=7 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and J. Principe. On the scalability of inexact balancing domain decomposition by constraints with overlapped coarse/fine corrections. <em>Parallel Computing</em>, 50:1‚Äì24, 2015. <a href="https://dx.doi.org/ 10.1016/j.parco.2015.09.004">DOI: 10.1016/j.parco.2015.09.004</a>.</p> </ol> <a id=badia2019  class=anchor ></a> <ol start=8 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and H. Nguyen. Balancing domain decomposition by constraints associated with subobjects. <em>Applied Mathematics Letters</em>, 87:93‚Äì100, 2019. <a href="https://dx.doi.org/ 10.1016/J.AML.2018.07.033">DOI: 10.1016/J.AML.2018.07.033</a>.</p> </ol> <a id=badia_aggregated_2017  class=anchor ></a> <ol start=9 > <li><p>S. Badia, F. Verdugo, and <strong>A. F. Mart√≠n</strong>. The aggregated unfitted finite element method for elliptic problems. <em>Computer Methods in Applied Mechanics and Engineering</em>, 336:533‚Äì553, 2018. <a href="https://dx.doi.org/ 10.1016/j.cma.2018.03.022">DOI: 10.1016/j.cma.2018.03.022</a>.</p> </ol> <a id=badia_stokes_2018  class=anchor ></a> <ol start=10 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong> and F. Verdugo. Mixed Aggregated Finite Element Methods for the Unfitted Discretization of the Stokes Problem. <em>SIAM Journal on Scientific Computing</em>, 40&#40;6&#41;:B1541‚ÄìB1576, 2018. <a href="https://dx.doi.org/ 10.1137/18M1185624">DOI: 10.1137/18M1185624</a>.</p> </ol> <a id=olm2019  class=anchor ></a> <ol start=11 > <li><p>M. Olm, S. Badia, and <strong>A. F. Mart√≠n</strong>. Simulation of High Temperature Superconductors and experimental validation. <em>Computer Physics Communications</em>, 237:154‚Äì167, 2019. <a href="https://dx.doi.org/ 10.1016/J.CPC.2018.11.021">DOI: 10.1016/J.CPC.2018.11.021</a>.</p> </ol> <a id=badia2019maxwell  class=anchor ></a> <ol start=12 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and M. Olm. Scalable solvers for complex electromagnetics problems. <em>Finite Elements in Analysis and Design</em>, 161:16‚Äì31, 2019. <a href="https://dx.doi.org/ 10.1016/J.FINEL.2019.04.003">DOI: 10.1016/J.FINEL.2019.04.003</a>.</p> </ol> <a id=verdugo2019  class=anchor ></a> <ol start=13 > <li><p>F. Verdugo, <strong>A. F. Mart√≠n</strong>, and S. Badia. Distributed-memory parallelization of the aggregated unfitted finite element method. Computer Methods in Applied Mechanics and Engineering, 357:112583, 2019. <a href="https://dx.doi.org/ 10.1016/j.cma.2019.112583">DOI: 10.1016/j.cma.2019.112583</a>.</p> </ol> <a id=badia2020fempar  class=anchor ></a> <ol start=14 > <li><p>S. Badia and <strong>A. F. Mart√≠n</strong>. A tutorial-driven introduction to the parallel finite element library FEMPAR v1.0.0. <em>Computer Physics Communications</em>, 248:107059, 2020. <a href="https://dx.doi.org/ 10.1016/j.cpc.2019.107059">DOI: 10.1016/j.cpc.2019.107059</a></p> </ol> <a id=olm2019b  class=anchor ></a> <ol start=15 > <li><p>M. Olm, S. Badia, and <strong>A. F. Mart√≠n</strong>. On a general implementation of h- and p-adaptive curl-conforming finite elements. <em>Advances in Engineering Software</em>, 132:74‚Äì91, 2019. <a href="https://dx.doi.org/ 10.1016/J.ADVENGSOFT.2019.03.006">DOI: 10.1016/J.ADVENGSOFT.2019.03.006</a>.</p> </ol> <a id=badia_fempar_2017  class=anchor ></a> <ol start=16 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong> and J. Principe. FEMPAR: An Object-Oriented Parallel Finite Element Framework. <em>Archives of Computational Methods in Engineering</em>, 25&#40;2&#41;:195-271, 2018. <a href="https://dx.doi.org/ 10.1007/s11831-017-9244-1">DOI: 10.1007/s11831-017-9244-1</a>.</p> </ol> <a id=badia_implementation_2013  class=anchor ></a> <ol start=17 > <li><p>S. Badia, <strong>A. F. Mart√≠n</strong>, and J. Principe. Implementation and scalability analysis of balancing domain decomposition methods. <em>Archives of Computational Methods in Engineering</em>, 20&#40;3&#41;:239‚Äì262, 2013. <a href="https://dx.doi.org/ 10.1007/s11831-013-9086-4">DOI: 10.1007/s11831-013-9086-4</a>.</p> </ol> <a id=neiva2020  class=anchor ></a> <ol start=18 > <li><p>E. Neiva, M. Chiumenti, M. Cervera, E. Salsi, G. Piscopo, S. Badia, <strong>A. F. Mart√≠n</strong>, Z. Chen, C. Lee, and C. Davies. Numerical modelling of heat transfer and experimental validation in powder-bed fusion with the virtual domain approximation. <em>Finite Elements in Analysis and Design</em>, 168, 2020. <a href="https://dx.doi.org/ 10.1016/j.finel.2019.103343">DOI: 10.1016/j.finel.2019.103343</a></p> </ol> <a id=neiva2019a  class=anchor ></a> <ol start=19 > <li><p>E. Neiva, S. Badia, <strong>A. F. Mart√≠n</strong>, and M. Chiumenti. A scalable parallel finite element framework for growing geometries. Application to metal additive manufacturing. <em>International Journal for Numerical Methods in Engineering</em>, <a href="https://dx.doi.org/ 10.1002/nme.6085">DOI: 10.1002/nme.6085</a>.</p> </ol></div> <p>‚Äì&gt;</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Alberto F Mart√≠n. Last modified: April 26, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div> <!-- end of class page-wrap-->